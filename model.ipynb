{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import math\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda NVIDIA GeForce GTX 1080 Ti\n",
      "1.12.1+cu116\n",
      "0.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(device)\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MusicSheetDataSet\n",
    "import random\n",
    "\n",
    "def area(box):\n",
    "    return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "def crop(image, target, region, dataset):\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for annotation in target:\n",
    "        # print(annotation)\n",
    "        orig_box = [x for x in annotation['a_bbox']]\n",
    "        new_box = [\n",
    "            max(orig_box[0], region[0]) - region[0],\n",
    "            max(orig_box[1], region[1]) - region[1],\n",
    "            min(orig_box[2], region[2]) - region[0],\n",
    "            min(orig_box[3], region[3]) - region[1],\n",
    "        ]\n",
    "\n",
    "        if new_box[0] >= new_box[2] or new_box[1] >= new_box[3] or area(new_box) < area(orig_box) * 0.5:\n",
    "            continue\n",
    "\n",
    "        for cat_id in annotation['cat_id']:\n",
    "            if (cat_id == None):\n",
    "                continue\n",
    "\n",
    "            category = dataset.get_category(cat_id)\n",
    "            if (category['annotation_set'] != 'deepscores'):\n",
    "                continue\n",
    "\n",
    "            if (category['name'] in {'stem', 'ledgerLine'}):\n",
    "                break\n",
    "            # if (category['name'] in oneset):\n",
    "            labels.append(int(cat_id))\n",
    "            boxes.append(new_box)\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(image[region[1] : region[3], region[0] : region[2]]).div(255).unsqueeze(0),\n",
    "        {\n",
    "            'boxes': torch.tensor(boxes),\n",
    "            'labels': torch.tensor(labels),\n",
    "        }\n",
    "    )\n",
    "\n",
    "def transform(images, targets, dataset):\n",
    "    image_res = []\n",
    "    target_res = []\n",
    "    for image, target in zip(images, targets):\n",
    "        height, width = image.shape\n",
    "\n",
    "        x = random.randrange(0, width // 2)\n",
    "        y = random.randrange(0, height // 2)\n",
    "        region = [x, y, x + width // 2, y + height // 2]\n",
    "\n",
    "        i, t = crop(image, target, region, dataset)\n",
    "\n",
    "        if t['boxes'].shape[0] == 0:\n",
    "            region = [0, 0, width // 2, height // 2]\n",
    "            i, t = crop(image, target, region, dataset)\n",
    "        \n",
    "        image_res.append(i)\n",
    "        target_res.append(t)\n",
    "    return image_res, target_res\n",
    "\n",
    "dataset = MusicSheetDataSet(\"ds2_dense\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music\n",
    "\n",
    "class MusicSymbolDetector:\n",
    "    def __init__(self):\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(\n",
    "            pretrained=True,\n",
    "            num_classes=137,\n",
    "            min_size=1024,\n",
    "            max_size=1024,\n",
    "            box_detections_per_img=300\n",
    "        )\n",
    "\n",
    "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        self.optimizer = torch.optim.Adam(params)\n",
    "        self.epoch = 0\n",
    "        self.loss = 0\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        # split image into 6 smaller image for accurate small object detection\n",
    "        height, width = image.shape\n",
    "        box_size = int(width * 0.55)\n",
    "\n",
    "        x_starts = [0, width - box_size]\n",
    "        y_starts = [0, (height - box_size) // 2, height - box_size]\n",
    "\n",
    "        overlap_size_x = int(2 * box_size - width)\n",
    "        overlap_size_y = int(3 * box_size - height) // 2\n",
    "\n",
    "        results = []\n",
    "\n",
    "        x_intersects: list[music.Label] = []\n",
    "        y_intersects: list[music.Label] = []\n",
    "        \n",
    "        def check_x_intersects(x, label: music.Label):\n",
    "            if label.x_min() <= x + overlap_size_x:\n",
    "                for l in x_intersects:\n",
    "                    if label.intersects_with(l) and label.name == l.name:\n",
    "                        label.box = label.union(l).box\n",
    "                        x_intersects.remove(l)\n",
    "            # should be checked against the next x area\n",
    "            if label.x_max() >= x + box_size - overlap_size_x:\n",
    "                x_intersects_next.append(label)\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        def check_y_intersect(y, label: music.Label):\n",
    "            if label.y_min() <= y + overlap_size_y:\n",
    "                for l in y_intersects:\n",
    "                    if label.intersects_with(l) and label.name == l.name:\n",
    "                        label.box = label.union(l).box\n",
    "                        y_intersects.remove(l)\n",
    "            # should be checked against the next y area\n",
    "            if label.y_max() >= y + box_size - overlap_size_y:\n",
    "                y_intersects_next.append(label)\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        for y in y_starts:\n",
    "            y_intersects_next = []\n",
    "            for x in x_starts:\n",
    "                x_intersects_next = []\n",
    "\n",
    "                # inference on each sub image\n",
    "                i = torch.tensor(image[y : y + box_size, x : x + box_size]).div(255).unsqueeze_(0).to(device)\n",
    "                result = self.model([i])[0]\n",
    "\n",
    "                result[\"boxes\"][:, (0, 2)] += x\n",
    "                result[\"boxes\"][:, (1, 3)] += y\n",
    "\n",
    "                for box, label in zip(result[\"boxes\"].detach().cpu(), result[\"labels\"].detach().cpu()):\n",
    "                    label = music.Label(\n",
    "                        dataset.get_category(label.item()),\n",
    "                        box,\n",
    "                    )\n",
    "\n",
    "                    if check_x_intersects(x, label): continue\n",
    "                    if check_y_intersect(y, label): continue\n",
    "                    results.append(label)\n",
    "                \n",
    "                # all labels that didn't have intersect are added\n",
    "                for label in x_intersects:\n",
    "                    if check_y_intersect(y, label): continue\n",
    "                    results.append(label)\n",
    "                \n",
    "                x_intersects = x_intersects_next\n",
    "            \n",
    "            # all labels at the end x edge\n",
    "            for label in x_intersects:\n",
    "                if check_y_intersect(y, label): continue\n",
    "                results.append(label)\n",
    "\n",
    "            x_intersects = []\n",
    "            results.extend(y_intersects)\n",
    "\n",
    "        # all labels at the end y edge\n",
    "        results.extend(y_intersects)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"epoch\": self.epoch,\n",
    "            \"loss\": self.loss,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path = None):\n",
    "        if path == None:\n",
    "            path = self\n",
    "            self = MusicSymbolDetector()\n",
    "        \n",
    "        data = torch.load(path)\n",
    "        self.model.load_state_dict(data['model'])\n",
    "        self.optimizer.load_state_dict(data['optimizer'])\n",
    "        current_epoch = data['epoch']\n",
    "        \n",
    "        print(\"loaded model at epoch: {}, loss: {}\".format(current_epoch, data['loss']))\n",
    "\n",
    "        # move optimizer to cuda\n",
    "        for state in self.optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def train(self, dataset : MusicSheetDataSet, epochs : int = 1, transform = None):\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        data_count = len(dataset)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            all_losses = 0\n",
    "            all_losses_dict = {}\n",
    "\n",
    "            dataloader = DataLoader(\n",
    "                dataset,\n",
    "                collate_fn=lambda x : zip(*x),\n",
    "                shuffle = True\n",
    "            )\n",
    "\n",
    "            for images, targets in tqdm(dataloader):\n",
    "                if transform != None:\n",
    "                    images, targets = transform(images, targets, dataset)\n",
    "                \n",
    "                images = [image.to(device) for image in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                loss_dict: dict[str, torch.Tensor] = self.model(images, targets) # the model computes the loss automatically if we pass in targets\n",
    "\n",
    "                losses: torch.Tensor = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                loss_value = losses.item()\n",
    "                all_losses += loss_value\n",
    "                \n",
    "                for k, v in loss_dict.items():\n",
    "                    if k not in all_losses_dict:\n",
    "                        all_losses_dict[k] = 0\n",
    "                    all_losses_dict[k] += v\n",
    "                \n",
    "                if not math.isfinite(loss_value):\n",
    "                    print(f\"Loss is {loss_value}, stopping trainig\") # train if loss becomes infinity\n",
    "                    print(loss_dict)\n",
    "                    sys.exit(1)\n",
    "                \n",
    "                losses.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            self.epoch += 1\n",
    "            self.loss = all_losses / data_count\n",
    "            print(\"Epoch {:>3}, lr: {:.6f}, loss: {:.6f}, {}\".format(\n",
    "                self.epoch,\n",
    "                self.optimizer.param_groups[0]['lr'], \n",
    "                self.loss,\n",
    "                ', '.join(\"{}: {:.6f}\".format(k, v / data_count) for k, v in all_losses_dict.items()),\n",
    "            ))\n",
    "\n",
    "            self.save(f\"model/{self.epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model at epoch: 10, loss: 0.36558048376777974\n"
     ]
    }
   ],
   "source": [
    "detector = MusicSymbolDetector.load(\"model/10\")\n",
    "# detector.train(dataset, 10, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector = MusicSymbolDetector.load(\"model/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     boxes\u001b[38;5;241m.\u001b[39mappend(label\u001b[38;5;241m.\u001b[39mbox)\n\u001b[0;32m     17\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(label\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m---> 19\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(draw_bounding_boxes(torch\u001b[38;5;241m.\u001b[39mtensor(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m)\u001b[49m, labels)\u001b[38;5;241m.\u001b[39mmoveaxis(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg2.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# img = cv2.imread(\"ds2_dense/images/lg-900267602436792595-aug-gutenberg1939--page-4.png\", cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.imread(\"sheets/bohemia rhapsody.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# img, res = dataset[266]\n",
    "# load = torch.load(\"fasterrcnn/1\")\n",
    "# model.load_state_dict(load['model'])\n",
    "# optimizer.load_state_dict(load['optimizer'])\n",
    "# load(\"fasterrcnn/1024-1\")\n",
    "\n",
    "res = detector(img)\n",
    "\n",
    "boxes = []\n",
    "labels = []\n",
    "\n",
    "for label in res:\n",
    "    boxes.append(label.box if torch.is_tensor(label.box) else torch.stack(label.box))\n",
    "    labels.append(label.name)\n",
    "\n",
    "plt.imshow(draw_bounding_boxes(torch.tensor(img).unsqueeze(0), torch.stack(boxes), labels).moveaxis(0, 2))\n",
    "plt.savefig(\"img2.png\", dpi=800)\n",
    "\n",
    "\n",
    "# print(img.shape)\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "# results = HRNetBackbbone().to(device)(img[:, 0:1280, 0:1280].unsqueeze(0))\n",
    "# results = model.detect(img)\n",
    "\n",
    "# print(results)\n",
    "\n",
    "# x = results['0'].sum(1).moveaxis(0, 2).detach().cpu()\n",
    "# print(x.shape)\n",
    "# plt.imshow(x)\n",
    "# plt.savefig(\"img.png\", dpi=800)\n",
    "\n",
    "# model.model.backbone = HRNetBackbbone()\n",
    "\n",
    "\n",
    "# TODO: validation, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4148148148148147 1.4138398914518318\n"
     ]
    }
   ],
   "source": [
    "from dataset import MusicSheetDataSet\n",
    "\n",
    "max_asp = 0\n",
    "min_asp = 100\n",
    "\n",
    "def trans(image, target):\n",
    "    global max_asp, min_asp\n",
    "\n",
    "    y, x = image.shape\n",
    "\n",
    "    asp = y / x\n",
    "    if asp > max_asp:\n",
    "        max_asp = asp\n",
    "    if asp < min_asp:\n",
    "        min_asp = asp\n",
    "\n",
    "\n",
    "x = MusicSheetDataSet(\"ds2_dense\", \"train\", trans)\n",
    "\n",
    "for i in x:\n",
    "    pass\n",
    "\n",
    "print(max_asp, min_asp)\n",
    "\n",
    "# image, target = dataset[0]\n",
    "# print(image.shape)\n",
    "# _, x, y = image.shape\n",
    "\n",
    "\n",
    "# scaleX = 800 / x\n",
    "# scaleY = 800 / y\n",
    "\n",
    "# # image = cv2.resize(image.mul(255).type(torch.uint8).numpy(), dsize=(800, 800))\n",
    "\n",
    "# # target = model(image.unsqueeze(0).to(device))[0]\n",
    "\n",
    "# plt.imshow(draw_bounding_boxes(\n",
    "#     image.mul(255).type(torch.uint8), \n",
    "#     torch.concat([torch.tensor([b[0] * scaleX, b[1] * scaleY, b[2] * scaleX, b[3] * scaleY]).unsqueeze(0) for b in target['boxes']]), \n",
    "#     [oneset_rev[x.item()] for x in target['labels']]\n",
    "# ).moveaxis(0, 2))\n",
    "# plt.savefig(\"img.png\", dpi=800)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
